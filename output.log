(base) llm@llm-server:~/model/deepseek/DeepSeek-R1-0528-Qwen3-8B$ docker run \
  -d \
  --gpus all \
  --name coder \
  --shm-size 64g \
  --ulimit memlock=-1 \
  --restart always \
  --ipc=host \
  -v /home/llm/model/deepseek/DeepSeek-R1-0528-Qwen3-8B:/models \
  -p 8000:8000 \
  -e CUDA_MODULE_LOADING=LAZY \
  vllm/vllm-openai:v0.8.5 \
  --model /models \
  --served-model-name coder \
  --tensor-parallel-size 4 \
  --gpu-memory-utilization 0.93 \
  --dtype float16 \
  --max-model-len 65536 \
  --trust-remote-code \
  --load-format safetensors \
  --swap-space 32 \
  --enforce-eager \
  --max-num-batched-tokens 8192 \
  --chat-template /models/qwen3_programming.jinja
8f0cffac9c7e35503db55266fd0d8c6bb6ea40e138c2ac72964a87322b7d0c1c
(base) llm@llm-server:~/model/deepseek/DeepSeek-R1-0528-Qwen3-8B$ docker logs -f coder
INFO 06-07 03:27:19 [__init__.py:239] Automatically detected platform cuda.
INFO 06-07 03:27:23 [api_server.py:1043] vLLM API server version 0.8.5
INFO 06-07 03:27:23 [api_server.py:1044] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template='/models/qwen3_programming.jinja', chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='/models', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, allowed_local_media_path=None, load_format='safetensors', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='float16', max_model_len=65536, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=4, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.93, swap_space=32.0, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=True, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=['coder'], qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=8192, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}
WARNING 06-07 03:27:23 [config.py:2972] Casting torch.bfloat16 to torch.float16.
INFO 06-07 03:27:32 [config.py:717] This model supports multiple tasks: {'score', 'reward', 'embed', 'classify', 'generate'}. Defaulting to 'generate'.
WARNING 06-07 03:27:32 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
WARNING 06-07 03:27:32 [arg_utils.py:1525] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 06-07 03:27:32 [config.py:1770] Defaulting to use mp for distributed inference
INFO 06-07 03:27:32 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 06-07 03:27:32 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 06-07 03:27:32 [api_server.py:246] Started engine process with PID 131
INFO 06-07 03:27:36 [__init__.py:239] Automatically detected platform cuda.
INFO 06-07 03:27:38 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5) with config: model='/models', speculative_config=None, tokenizer='/models', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=65536, download_dir=None, load_format=LoadFormat.SAFETENSORS, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=coder, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[],"max_capture_size":0}, use_cached_outputs=True, 
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}
WARNING 06-07 03:27:39 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 28 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 06-07 03:27:40 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 06-07 03:27:40 [cuda.py:289] Using XFormers backend.
INFO 06-07 03:27:43 [__init__.py:239] Automatically detected platform cuda.
INFO 06-07 03:27:43 [__init__.py:239] Automatically detected platform cuda.
INFO 06-07 03:27:43 [__init__.py:239] Automatically detected platform cuda.
(VllmWorkerProcess pid=197) INFO 06-07 03:27:45 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks
(VllmWorkerProcess pid=198) INFO 06-07 03:27:45 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks
(VllmWorkerProcess pid=196) INFO 06-07 03:27:45 [multiproc_worker_utils.py:225] Worker ready; awaiting tasks
(VllmWorkerProcess pid=197) INFO 06-07 03:27:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
(VllmWorkerProcess pid=197) INFO 06-07 03:27:46 [cuda.py:289] Using XFormers backend.
(VllmWorkerProcess pid=198) INFO 06-07 03:27:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
(VllmWorkerProcess pid=198) INFO 06-07 03:27:46 [cuda.py:289] Using XFormers backend.
(VllmWorkerProcess pid=196) INFO 06-07 03:27:46 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
(VllmWorkerProcess pid=196) INFO 06-07 03:27:46 [cuda.py:289] Using XFormers backend.
(VllmWorkerProcess pid=196) INFO 06-07 03:27:48 [utils.py:1055] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=196) INFO 06-07 03:27:48 [pynccl.py:69] vLLM is using nccl==2.21.5
(VllmWorkerProcess pid=198) INFO 06-07 03:27:48 [utils.py:1055] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=198) INFO 06-07 03:27:48 [pynccl.py:69] vLLM is using nccl==2.21.5
(VllmWorkerProcess pid=197) INFO 06-07 03:27:48 [utils.py:1055] Found nccl from library libnccl.so.2
(VllmWorkerProcess pid=197) INFO 06-07 03:27:48 [pynccl.py:69] vLLM is using nccl==2.21.5
INFO 06-07 03:27:48 [utils.py:1055] Found nccl from library libnccl.so.2
INFO 06-07 03:27:48 [pynccl.py:69] vLLM is using nccl==2.21.5
(VllmWorkerProcess pid=198) WARNING 06-07 03:27:49 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
(VllmWorkerProcess pid=196) WARNING 06-07 03:27:49 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
(VllmWorkerProcess pid=197) WARNING 06-07 03:27:49 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
WARNING 06-07 03:27:49 [custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.
INFO 06-07 03:27:49 [shm_broadcast.py:266] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cead9a0d'), local_subscribe_addr='ipc:///tmp/932aefa4-2389-4969-bca0-9ecb6bd70d3f', remote_subscribe_addr=None, remote_addr_ipv6=False)
(VllmWorkerProcess pid=198) INFO 06-07 03:27:49 [parallel_state.py:1004] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3
(VllmWorkerProcess pid=196) INFO 06-07 03:27:49 [parallel_state.py:1004] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1
(VllmWorkerProcess pid=197) INFO 06-07 03:27:49 [parallel_state.py:1004] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2
INFO 06-07 03:27:49 [parallel_state.py:1004] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-07 03:27:49 [model_runner.py:1108] Starting to load model /models...
(VllmWorkerProcess pid=197) INFO 06-07 03:27:49 [model_runner.py:1108] Starting to load model /models...
(VllmWorkerProcess pid=198) INFO 06-07 03:27:49 [model_runner.py:1108] Starting to load model /models...
(VllmWorkerProcess pid=196) INFO 06-07 03:27:49 [model_runner.py:1108] Starting to load model /models...
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.49s/it]
(VllmWorkerProcess pid=197) INFO 06-07 03:27:54 [loader.py:458] Loading weights took 4.70 seconds
(VllmWorkerProcess pid=197) INFO 06-07 03:27:54 [model_runner.py:1140] Model loading took 3.8459 GiB and 4.913491 seconds
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.66s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:05<00:00,  2.63s/it]

(VllmWorkerProcess pid=198) INFO 06-07 03:27:54 [loader.py:458] Loading weights took 5.34 seconds
INFO 06-07 03:27:54 [loader.py:458] Loading weights took 5.34 seconds
(VllmWorkerProcess pid=196) INFO 06-07 03:27:54 [loader.py:458] Loading weights took 5.35 seconds
(VllmWorkerProcess pid=198) INFO 06-07 03:27:54 [model_runner.py:1140] Model loading took 3.8459 GiB and 5.548199 seconds
INFO 06-07 03:27:55 [model_runner.py:1140] Model loading took 3.8459 GiB and 5.549861 seconds
(VllmWorkerProcess pid=196) INFO 06-07 03:27:55 [model_runner.py:1140] Model loading took 3.8459 GiB and 5.562168 seconds
(VllmWorkerProcess pid=198) INFO 06-07 03:28:01 [worker.py:287] Memory profiling takes 5.90 seconds
(VllmWorkerProcess pid=198) INFO 06-07 03:28:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.49GiB) x gpu_memory_utilization (0.93) = 19.98GiB
(VllmWorkerProcess pid=198) INFO 06-07 03:28:01 [worker.py:287] model weights take 3.85GiB; non_torch_memory takes 0.18GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 15.49GiB.
(VllmWorkerProcess pid=196) INFO 06-07 03:28:01 [worker.py:287] Memory profiling takes 5.95 seconds
(VllmWorkerProcess pid=196) INFO 06-07 03:28:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.49GiB) x gpu_memory_utilization (0.93) = 19.98GiB
(VllmWorkerProcess pid=196) INFO 06-07 03:28:01 [worker.py:287] model weights take 3.85GiB; non_torch_memory takes 0.19GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 15.49GiB.
(VllmWorkerProcess pid=197) INFO 06-07 03:28:01 [worker.py:287] Memory profiling takes 5.94 seconds
(VllmWorkerProcess pid=197) INFO 06-07 03:28:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.49GiB) x gpu_memory_utilization (0.93) = 19.98GiB
(VllmWorkerProcess pid=197) INFO 06-07 03:28:01 [worker.py:287] model weights take 3.85GiB; non_torch_memory takes 0.18GiB; PyTorch activation peak memory takes 0.46GiB; the rest of the memory reserved for KV Cache is 15.49GiB.
INFO 06-07 03:28:01 [worker.py:287] Memory profiling takes 6.04 seconds
INFO 06-07 03:28:01 [worker.py:287] the current vLLM instance can use total_gpu_memory (21.48GiB) x gpu_memory_utilization (0.93) = 19.98GiB
INFO 06-07 03:28:01 [worker.py:287] model weights take 3.85GiB; non_torch_memory takes 0.20GiB; PyTorch activation peak memory takes 1.45GiB; the rest of the memory reserved for KV Cache is 14.49GiB.
INFO 06-07 03:28:01 [executor_base.py:112] # cuda blocks: 26370, # CPU blocks: 58254
INFO 06-07 03:28:01 [executor_base.py:117] Maximum concurrency for 65536 tokens per request: 6.44x
INFO 06-07 03:28:26 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 31.52 seconds
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}
WARNING 06-07 03:28:28 [api_server.py:945] Using supplied chat template: {# Enhanced template for Qwen3 optimized for programming tasks #}
WARNING 06-07 03:28:28 [api_server.py:945] {% if messages[0]['role'] == 'system' %}
WARNING 06-07 03:28:28 [api_server.py:945]     {% set loop_messages = messages[1:] %}
WARNING 06-07 03:28:28 [api_server.py:945]     {% set system_message = messages[0]['content'] %}
WARNING 06-07 03:28:28 [api_server.py:945] {% else %}
WARNING 06-07 03:28:28 [api_server.py:945]     {% set loop_messages = messages %}
WARNING 06-07 03:28:28 [api_server.py:945]     {% set system_message = "You are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies." %}
WARNING 06-07 03:28:28 [api_server.py:945] {% endif %}
WARNING 06-07 03:28:28 [api_server.py:945] 
WARNING 06-07 03:28:28 [api_server.py:945] {# Always include system message for programming optimization #}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>system
WARNING 06-07 03:28:28 [api_server.py:945] {{ system_message }}<|im_end|>
WARNING 06-07 03:28:28 [api_server.py:945] 
WARNING 06-07 03:28:28 [api_server.py:945] {% for message in loop_messages %}
WARNING 06-07 03:28:28 [api_server.py:945]     {% if message['role'] == 'user' %}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>user
WARNING 06-07 03:28:28 [api_server.py:945] {{ message['content'] }}<|im_end|>
WARNING 06-07 03:28:28 [api_server.py:945]     {% elif message['role'] == 'assistant' %}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>assistant
WARNING 06-07 03:28:28 [api_server.py:945] {{ message['content'] }}<|im_end|>
WARNING 06-07 03:28:28 [api_server.py:945]     {% elif message['role'] == 'tool' %}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>tool
WARNING 06-07 03:28:28 [api_server.py:945] {{ message['content'] }}<|im_end|>
WARNING 06-07 03:28:28 [api_server.py:945]     {% else %}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>{{ message['role'] }}
WARNING 06-07 03:28:28 [api_server.py:945] {{ message['content'] }}<|im_end|>
WARNING 06-07 03:28:28 [api_server.py:945]     {% endif %}
WARNING 06-07 03:28:28 [api_server.py:945] {% endfor %}
WARNING 06-07 03:28:28 [api_server.py:945] 
WARNING 06-07 03:28:28 [api_server.py:945] {% if add_generation_prompt %}
WARNING 06-07 03:28:28 [api_server.py:945] <|im_start|>assistant
WARNING 06-07 03:28:28 [api_server.py:945] {% endif %}
WARNING 06-07 03:28:28 [api_server.py:945] 
WARNING 06-07 03:28:28 [api_server.py:945] 
WARNING 06-07 03:28:28 [api_server.py:945] It is different from official chat template '/models'. This discrepancy may lead to performance degradation.
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}
Unrecognized keys in `rope_scaling` for 'rope_type'='yarn': {'attn_factor'}
INFO 06-07 03:28:28 [api_server.py:1090] Starting vLLM API server on http://0.0.0.0:8000
INFO 06-07 03:28:28 [launcher.py:28] Available routes are:
INFO 06-07 03:28:28 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD
INFO 06-07 03:28:28 [launcher.py:36] Route: /docs, Methods: GET, HEAD
INFO 06-07 03:28:28 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD
INFO 06-07 03:28:28 [launcher.py:36] Route: /redoc, Methods: GET, HEAD
INFO 06-07 03:28:28 [launcher.py:36] Route: /health, Methods: GET
INFO 06-07 03:28:28 [launcher.py:36] Route: /load, Methods: GET
INFO 06-07 03:28:28 [launcher.py:36] Route: /ping, Methods: GET, POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /tokenize, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /detokenize, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/models, Methods: GET
INFO 06-07 03:28:28 [launcher.py:36] Route: /version, Methods: GET
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/chat/completions, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/completions, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/embeddings, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /pooling, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /score, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/score, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/audio/transcriptions, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /rerank, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v1/rerank, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /v2/rerank, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /invocations, Methods: POST
INFO 06-07 03:28:28 [launcher.py:36] Route: /metrics, Methods: GET
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO 06-07 03:28:58 [chat_utils.py:397] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
INFO 06-07 03:28:58 [logger.py:39] Received request chatcmpl-96cba9382b424b60a5c791daa809b383: prompt: '\n<|im_start|>system\nYou are a programming assistant specialized in writing clean, efficient, and well-documented code. Provide direct code solutions without unnecessary explanations unless requested. Focus on best practices, optimal algorithms, and proper error handling. When multiple approaches exist, choose the most efficient one by default. Always include necessary imports and dependencies.<|im_end|>\n\n<|im_start|>user\n写一个Python函数计算斐波那契数列的第n项，要求使用动态规划优化性能<|im_end|>\n\n<|im_start|>assistant\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.2, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=65432, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.
INFO 06-07 03:28:58 [engine.py:310] Added request chatcmpl-96cba9382b424b60a5c791daa809b383.
INFO 06-07 03:29:03 [metrics.py:486] Avg prompt throughput: 19.7 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:08 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 15.3 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:13 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:18 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:23 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 19.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:28 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:33 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:38 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO 06-07 03:29:43 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 18.4 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
INFO:     172.17.0.1:56104 - "POST /v1/chat/completions HTTP/1.1" 200 OK
INFO 06-07 03:29:54 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 06-07 03:30:04 [metrics.py:486] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
